{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file intends to solve the following questions:\n",
    "    - get the vector of each influencers w/ previous saved model\n",
    "    - save the vectors\n",
    "    \n",
    "- https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/;\n",
    "- https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/\n",
    "- http://qpleple.com/topic-coherence-to-evaluate-topic-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hellofutrue/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os,glob\n",
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "# NLTK Stop words\n",
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'not', 'would', 'say', 'could', '_', 'be', 'know', 'good', 'go', 'get', 'do', 'done', 'try', 'many', 'some', 'nice', 'thank', 'think', 'see', 'rather', 'easy', 'easily', 'lot', 'lack', 'make', 'want', 'seem', 'run', 'need', 'even', 'right', 'line', 'even', 'also', 'may', 'take', 'come'])\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model from disk.\n",
    "lda_model =  models.LdaModel.load('lda_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous dictionary\n",
    "id2word = Dictionary.load_from_text('/Users/hellofutrue/Desktop/Insight/Python/Feb/dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data (each influencers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_influencers = pd.read_csv('/Users/hellofutrue/Desktop/Insight/Python/Feb/files/posts_influencers.csv')\n",
    "posts_influencers = posts_influencers.rename(index=str, columns={'Unnamed: 0': \"people\", '0': 'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sleepinthegardn',\n",
       " 'abigailratchford',\n",
       " 'adrianalima',\n",
       " 'alexandradaddario',\n",
       " 'alexa_bliss_wwe_',\n",
       " 'alexisren',\n",
       " 'aliciakeys',\n",
       " 'alissaviolet',\n",
       " 'alyraisman',\n",
       " 'amandacerny',\n",
       " 'amirahdyme',\n",
       " 'anacheri',\n",
       " 'anastasiya_kvitko',\n",
       " 'andrewsiwicki',\n",
       " 'angelcandices',\n",
       " 'antoniiogarza',\n",
       " 'ashleygraham',\n",
       " 'ashleynocera',\n",
       " 'augustalsina',\n",
       " 'badbunnypr',\n",
       " 'bakermayfield',\n",
       " 'banks',\n",
       " 'bellahadid',\n",
       " 'bensimmons',\n",
       " 'blacchyna',\n",
       " 'boogiecousins',\n",
       " 'britneyspears',\n",
       " 'brittanya187',\n",
       " 'brooklynbeckham',\n",
       " 'brunomars',\n",
       " 'bundleofbrittany',\n",
       " 'camila_cabello',\n",
       " 'caradelevingne',\n",
       " 'carlibel',\n",
       " 'carmenpritchett_',\n",
       " 'chanteljeffries',\n",
       " 'charliesangelll',\n",
       " 'chinamcclain',\n",
       " 'chrissyteigen',\n",
       " 'christymack',\n",
       " 'coledeboer',\n",
       " 'colleen',\n",
       " 'coltonlhaynes',\n",
       " 'coltonunderwood',\n",
       " 'cynsantana',\n",
       " 'daddyyankee',\n",
       " 'dannyduncan69',\n",
       " 'dearra',\n",
       " 'demirosemawby',\n",
       " 'djpaulyd',\n",
       " 'drayamichele',\n",
       " 'elliethumann',\n",
       " 'emmaroberts',\n",
       " 'emrata',\n",
       " 'enews',\n",
       " 'enikohart',\n",
       " 'ernst',\n",
       " 'forever21',\n",
       " 'gabi',\n",
       " 'gianna',\n",
       " 'gigihadid',\n",
       " 'gisele',\n",
       " 'gucci',\n",
       " 'haileybieber',\n",
       " 'hannahstocking',\n",
       " 'hardstop.lucas',\n",
       " 'hennessycarolina',\n",
       " 'hilaryduff',\n",
       " 'himynamesteee',\n",
       " 'iamerica_mena',\n",
       " 'iamyanetgarcia',\n",
       " 'iisuperwomanii',\n",
       " 'inayah_lamis',\n",
       " 'jailyneojeda',\n",
       " 'janetjackson',\n",
       " 'jayalvarrez',\n",
       " 'jccaylen',\n",
       " 'jeezy',\n",
       " 'jenafrumes',\n",
       " 'jenselter',\n",
       " 'jen_ny69',\n",
       " 'jeremyallenwhitefinally',\n",
       " 'jessicabiel',\n",
       " 'jessicakes33',\n",
       " 'jessicanigri',\n",
       " 'jessicasimpson',\n",
       " 'joelle_fletcher',\n",
       " 'joeyking',\n",
       " 'johnstamos',\n",
       " 'joiechavis',\n",
       " 'jojo_babie',\n",
       " 'jordynwoods',\n",
       " 'joselyncano',\n",
       " 'joynerlucas',\n",
       " 'juli.annee',\n",
       " 'jvn',\n",
       " 'jwoww',\n",
       " 'kalanihilliker',\n",
       " 'kalesalad',\n",
       " 'kandi',\n",
       " 'karliekloss',\n",
       " 'karrueche',\n",
       " 'kateupton',\n",
       " 'katyaelisehenry',\n",
       " 'keke',\n",
       " 'kiernanshipka',\n",
       " 'kingbach',\n",
       " 'kristenhanby',\n",
       " 'kuz',\n",
       " 'lacikaysomers',\n",
       " 'laurenconrad',\n",
       " 'letoyaluckett',\n",
       " 'lianev',\n",
       " 'lilyjcollins',\n",
       " 'lira_galore',\n",
       " 'lizzzak',\n",
       " 'ludacris',\n",
       " 'lynaritaa',\n",
       " 'maisie_williams',\n",
       " 'mandymooremm',\n",
       " 'melissamccarthy',\n",
       " 'mendeecees',\n",
       " 'michelle_lewin',\n",
       " 'mirandasingsofficial',\n",
       " 'missdollycastro',\n",
       " 'monicabrown',\n",
       " 'moriahmillss',\n",
       " 'mykie',\n",
       " 'nashgrier',\n",
       " 'nattyiceofficial',\n",
       " 'normani',\n",
       " 'ocasio2018',\n",
       " 'oliviaculpo',\n",
       " 'oliviajade',\n",
       " 'paulpogba',\n",
       " 'paulvedere',\n",
       " 'phfame',\n",
       " 'playmateiryna',\n",
       " 'pokimanelol',\n",
       " 'prettyboyfredo',\n",
       " 'princesslove',\n",
       " 'princess_of_da_south',\n",
       " 'realbarbarapalvin',\n",
       " 'realberniceburgos',\n",
       " 'realpaigewwe',\n",
       " 'relationships.usa',\n",
       " 'rickeythompson',\n",
       " 'ritaora',\n",
       " 'romeestrijd',\n",
       " 'rosiehw',\n",
       " 'rubyrose',\n",
       " 'russwest44',\n",
       " 'sabrinacarpenter',\n",
       " 'sadiesink_',\n",
       " 'samsmith',\n",
       " 'saquon',\n",
       " 'saraunderwood',\n",
       " 'sashapieterse',\n",
       " 'savmontano',\n",
       " 'shaymitchell',\n",
       " 'shitheadsteve',\n",
       " 'skaijackson',\n",
       " 'sofiarichie',\n",
       " 'sofiavergara',\n",
       " 'sommerray2',\n",
       " 'sophiet',\n",
       " 'stassiebaby',\n",
       " 'summermckeen',\n",
       " 'supremenewyork',\n",
       " 'swaggyp1',\n",
       " 'tanamongeau',\n",
       " 'taylor_hill',\n",
       " 'thatgirljaycole',\n",
       " 'thecameronboyce',\n",
       " 'thehughjackman',\n",
       " 'thekenyamoore',\n",
       " 'therealkylesister',\n",
       " 'therealmaryjblige',\n",
       " 'theshaderoom',\n",
       " 'timtebow',\n",
       " 'todderic_',\n",
       " 'tommiee_',\n",
       " 'toochi_kash',\n",
       " 'toribrixx',\n",
       " 'toyawright',\n",
       " 'trevornoah',\n",
       " 'twan',\n",
       " 'tylerbaltierramtv',\n",
       " 'tyleroakley',\n",
       " 'tylerrjoseph',\n",
       " 'tyrabanks',\n",
       " 'vanilladingdong',\n",
       " 'victoriajustice',\n",
       " 'vinnyguadagnino',\n",
       " 'virginia',\n",
       " 'voguemagazine',\n",
       " 'winnieharlow',\n",
       " 'wolfiecindy',\n",
       " 'yarashahidi',\n",
       " 'yoventura',\n",
       " 'zionlw10',\n",
       " '_colebennett_',\n",
       " '_emmachamberlain',\n",
       " 'adele',\n",
       " 'arianagrande',\n",
       " 'badgalriri',\n",
       " 'barackobama',\n",
       " 'beyonce',\n",
       " 'blakelively',\n",
       " 'bretmanrock',\n",
       " 'champagnepapi',\n",
       " 'chrisbrownofficial',\n",
       " 'daquan',\n",
       " 'daviddobrik',\n",
       " 'ddlovato',\n",
       " 'ethandolan',\n",
       " 'fuckjerry',\n",
       " 'iamcardib',\n",
       " 'jamescharles',\n",
       " 'jlo',\n",
       " 'joannagaines',\n",
       " 'justinbieber',\n",
       " 'justintimberlake',\n",
       " 'kendalljenner',\n",
       " 'kevinhart4real',\n",
       " 'khloekardashian',\n",
       " 'kimkardashian',\n",
       " 'kingjames',\n",
       " 'kourtneykardash',\n",
       " 'kyliejenner',\n",
       " 'liluzivert',\n",
       " 'lizakoshy',\n",
       " 'michelleobama',\n",
       " 'mileycyrus',\n",
       " 'nickiminaj',\n",
       " 'obj',\n",
       " 'postmalone',\n",
       " 'prattprattpratt',\n",
       " 'selenagomez',\n",
       " 'shanedawson',\n",
       " 'shawnmendes',\n",
       " 'sommerray',\n",
       " 'stephencurry30',\n",
       " 'theellenshow',\n",
       " 'therock',\n",
       " 'vancityreynolds',\n",
       " 'wherearetheavocados',\n",
       " 'willsmith',\n",
       " 'world_record_egg',\n",
       " 'zacefron',\n",
       " 'zendaya']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(posts_influencers['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = posts_influencers.content.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(dat):\n",
    "    # Tokenization\n",
    "    def sent_to_words(sentences):\n",
    "        for sentence in sentences:\n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "    \n",
    "    data_words = list(sent_to_words(dat))\n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "    \n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    # Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['com', 'bio','link','get','go'])\n",
    "    def remove_stopwords(texts):\n",
    "        return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    \n",
    "    def make_bigrams(texts):\n",
    "        return [bigram_mod[doc] for doc in texts]\n",
    "    def make_trigrams(texts):\n",
    "        return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "        \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "        texts_out = []\n",
    "        for sent in texts:\n",
    "            doc = nlp(\" \".join(sent)) \n",
    "            texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        return texts_out\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "    # Initialize spacy 'en' model, keeping only tagger component (for efficiency\n",
    "    # python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    other_texts = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    return(other_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = []\n",
    "for i in range(0, len(data)):\n",
    "    other_texts = preprocessing(data[i].split(\"\\',\"))\n",
    "    other_corpus = [id2word.doc2bow(text) for text in other_texts]\n",
    "    unseen_doc = other_corpus[0]\n",
    "    vector = lda_model[unseen_doc]\n",
    "    vecs.append(vector[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vecs reformatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicn = 10\n",
    "topic_name = ['Topic 1','Topic 2','Topic 3','Topic 4','Topic 5','Topic 6','Topic 7','Topic 8','Topic 9','Topic 10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = pd.DataFrame()\n",
    "for i in range(0, topicn):\n",
    "    p = [item[i] for item in vecs]\n",
    "    a = [item[1] for item in p]\n",
    "    df = pd.DataFrame(np.array(a).reshape(1,len(a)))\n",
    "    vector = vector.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = vector.T\n",
    "vector.columns = list(topic_name)\n",
    "vector.index = list(posts_influencers['people'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector.to_csv('/Users/hellofutrue/Desktop/Insight/Python/Feb/files/vector.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
